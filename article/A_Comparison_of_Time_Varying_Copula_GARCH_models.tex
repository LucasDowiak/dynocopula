\documentclass[12pt]{article}

\usepackage{amssymb}
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{threeparttable}
\usepackage{geometry}
\usepackage{pdflscape}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage[parfill]{parskip}

\newcommand{\Lagr}{\mathcal{L}}
\newcommand{\boldXi}{\hat{\boldsymbol{\xi}}}
\newcommand{\checkplus}[0]{\checkmark +}


\newtheorem{defn}{Definition}
\newtheorem{thm}{Theorem}
\setlength{\parindent}{0pt}

\graphicspath{{/Users/lucasdowiak/Git/dynocopula/article/images/}}

\usepackage[
  backend=biber,
  url=false,
  doi=true,
  style=numeric,
  citestyle=numeric,
  sorting=nty
]{biblatex}

\bibliography{A_Comparison_of_Time_Varying_Copula_GARCH_models.bib}
 
\title{A Comparison of Dynamic Copula-GARCH Models}
\author{Lucas C. Dowiak}

\begin{document}
 
\maketitle{}
 
PhD Program in Economics, City University of New York\smallskip, Graduate Center,
New York, NY, 10016, \textit{Email: ldowiak@gradcenter.cuny.edu}

\qquad

\begin{quotation}
\textbf{Abstract:} This essay uses three separate tools to model the evolution of dependence structure between major global currencies from 2000 to 2018. It uses the ARMA-GARCH approach to time series modeling with Copula theory and multi-regime time series models to provide a flexible framework to model the return series of financial assets. The framework is applied to the log returns of the exchange rates between the US Dollar and the European Euro, British Pound, and Japanese Yen from 2000 to 2018.
\end{quotation}

\vspace{1pt}

\begin{quotation}
\textbf{Keywords}: Copulas, financial crisis, foreign exchange, time-varying dependence
\textbf{JEL Classification}: C51, G01, G15, F31
\end{quotation}

\vspace{1pt}

\section{Introduction}

The field of international finance has a long-running concern regarding the distribution of asset returns. Harry Markowitz \cite{Markowitz_1952, Markowitz_1959} provided the launching pad for modern portfolio theory by using a Gaussian framework to model the returns for a portfolio composed of an arbitrary number of assets. Over the past few decades though, a consensus has conformed on the belief that assuming normality, or joint-normality, of asset returns is not always wise. Empirical evidence strongly supports the fact that returns for certain asset classes frequently exhibit traits that deviate from normality. Two famous articles \cite{Mandelbrot_1963} and \cite{Fama_1965} present evidence that the tails of the Gaussian distribution are much too narrow to account for the number of observations occurring at least two standard deviations from the mean. More recently, \cite{Longin_1996} abandons the use of elliptical distributions to describe extreme returns altogether. \cite{Ang_and_Chen_2002} and \cite{Kroner_and_Ng_1998} study asymmetric co-movements in the returns of certain equity assets. \cite{Ang_and_Chen_2002} documents stronger downside risk between a range of portfolios and the aggregate U.S. market than upside risk. \cite{Kroner_and_Ng_1998} find "significant asymmetric effects in both the variances and covariances" for large-firm and small-firm returns.

The assumption of a constant correlation has also been challenged. In a popular article, \cite{Longin_and_Solnik_1995} review the correlation of excess returns on equities for the countries of France, U.S., U.K., Sweden, Japan, Canada, and Germany using monthly data from 1960-1990. Their results suggest a rejection of a constant international conditional correlation. In a similar vein, \cite{Ramchand_and_Susmel_1998} use a Markov Transition ARCH model and find that the correlation between the U.S. and other foreign markets is significantly higher in times of elevated volatility. \cite{Rodriguez_2007} studies financial contagion during the Asian financial crisis (circa 1997) and the Mexican peso devaluation (circa 1994). By combining a Markov Transition ARCH model with copula-based distributions, \cite{Rodriguez_2007} documents a change in dependence between a high variance regime and a low variance regime.

In this article, we employ a framework that will allow asset returns to deviate from the normality assumption by distribution and by time-varying dependence. In particular, this article studies evolution of three exchange rate pairs from 2000 to 2018, a period of time that includes the effect the financial crisis of 2007-2008 had on the foreign exchange market. To incorporate a degree of flexibility regarding the distribution of returns, copula theory will be applied to daily exchange rate data for some of the world's most influential currencies: the US Dollar, Japanese Yen, European Euro, and the British Pound. According to the 2010 Triennial Central Bank Survey conducted by the Bank of International Settlements, these currencies account for 78.0\% of all foreign exchange turnover in 2010. To capture potential breaks in the dependence over time, we estimate and compare three popular multi-regime models: a Markov Transition model, a Smooth Transition model, and a Sequential Break Point model.

Daily data is required since one focus of the article concentrates on estimating the dependence between observations in the extremes of the joint distribution. Weekly or monthly data will lead to a sample size that is too small to populate the tails, leading to imprecise estimates of the dependence and distributional parameters. The source for these series will be the Bank of England, whose database on historical exchange rates includes many of the world's currencies and is easy to access. The timeframe under review will be restricted to January 4, 2000 to October 4, 2018.

The layout for the rest of the paper is as follows. Section 2 discusses all relevant copula theory and important definitions. Section 3 provides the details of three popular multi-regime time series models while Section 4 focuses on estimating an ARMA-GARCH model for the exchange rates of the world's four most used currencies, by volume. Section 5 reviews the evolution of dependence between the major foreign exchange rates during the timeframe under review. Section 6 offers some concluding remarks.

\section{Copula Theory}

In this section, we briefly discuss the necessary copula theory and definitions that are put to use in this paper. Since our analysis pertains exclusively to bivariate distributions, all theory will be presented in the bivariate case. An accessibly treatment of basic copula theory can be found in \cite{Nelsen_2007} while additional topics on multivariate dependence can be found in \cite{Joe_1997}. For the individual in need of a concise introduction to the topic, the articles by \cite{Embrechts_et_al_2003} and \cite{Genest_and_Favre_2007} are a great entry point.

\subsection{Sklar's Theorem}

The term 'copula' has already been used various times in the introduction. To start, we provide a formal definition:

\begin{defn} \label{defn:copula}
	A bivariate copula is a function $C$ from $\left[0,1\right]^{2}$ to $\left[0,1\right]$ that satisfies (1) For every $u,w$ in $\left[0,1\right] :C\left(u,0\right) = C\left(0,w\right) = 0;$ (2) For every $u,w$ in $\left[0,1\right] :C\left(u,1\right) = u\ \ $and $\ C\left(1,w\right) = w;$ and (3) For every $u_{1}, u_{2}, w_{1}, w_{2}$ in $\left[0,1\right]$ such that $u_{1} \leq u_{2}$ and $w_{1} \leq w_{2}:C\left(u_{2}, w_{2}\right) - C\left(u_{1}, w_{2}\right) - C\left(u_{2}, w_{1}\right) + C\left(u_{1}, w_{1}\right)$ $\geq 0$. $_{\blacksquare}$
\end{defn}

Taken together, properties (1) - (3) imply that a copula is a nondecreasing function in each argument, a quality shared by all distribution functions. It is, in fact, helpful to think of copulas as distribution functions whose domain is restricted to the unit square---or the unit hypercube in higher dimensions. With this in mind, we present Sklar's theorem, the foundation for empirical applications of copulas.

\begin{thm}[Sklar, 1959] \label{thm:skl}
	Let $F_{XY}$ be a joint distribution function with margins $F_{X}$ and $F_{Y}$. Then there exists a copula $C$ such that for all $x,y$ in $\bar{\mathbb{R}}$,

	\begin{equation} \label{eq:sklars}
		F_{XY}\left( x,y\right) =C\left(F_{X}\left(x\right), F_{Y}\left(y\right)\right) = C\left(u,w\right)
	\end{equation}

	If $F_{X}$ and $F_{Y}$ are continuous, then $C$ is unique; otherwise, $C$ is uniquely determined on $RanX\times RanY$. Conversely, if $C$ is a copula and $F_{X}$ and $F_{Y}$ are distribution functions, then the function $F_{XY}$ defined by equation (\ref{eq:sklars}) is a joint distribution function with margins $F_{X}$ and $F_{Y}$. $_{\blacksquare}$
\end{thm}

The copula approach to multivariate distributions first transforms each marginal distribution from the extended real line $\left(\bar{\mathbb{R}}\right)$ to the unit interval through the \textit{probability integral transform} (PIT). First investigated by \cite{Rosenblatt_1952}, this transform states that if $X$ is a continuous random variable with density $f_{X}$, then the random variable $U$ defined by

\begin{equation} \label{eqn:PIT}
	u=\int_{-\infty}^{x}f_{X}\left(t\right) dt=F_{X}\left(x\right)
\end{equation}

is identically and independently distributed (iid) uniformly on the unit interval. In the bivariate case, this transformation shifts the domain of analysis from the extended real plane to the unit square. A copula is then chosen to install a dependence structure between the two marginal distributions. Sklar's theorem confirms the existence of a copula $C$ whose value $C\left(u,w\right) = C\left(F_{X}\left(x\right), F_{Y}\left(y\right)\right)$ provides the joint probability $P\left[U < u,~W < w\right] = P\left[X < x,~Y < y\right]$. In application, the copula approach allows us to separate a joint distribution $F_{XY}$ into marginal distributions $F_{X},F_{Y}$ and a dependence function $C$. The dependence between the marginal distributions $F_{X},F_{Y}$ is completely governed by the copula---and just as Sklar's theorem states---we can model this dependence separately.

\subsection{Patton's Extension}

Taking inspiration from \cite{Hansen_1994} and their work on time-varying density estimation, \cite{Patton_2006} extends Sklar's original theorem to the case of conditional distributions. This extension paves the way for copulas with time-varying parameterizations. In the same manner as Theorem~\ref{thm:skl}, \cite{Patton_2006} decomposes the conditional distribution $\left(X,Y\right) | Z$ into conditional marginal distributions and the conditional copula.


\begin{thm}[Patton, 2006] \label{thm:cond_skl}
	Let $F_{X|Z}(\cdot|z)$ be the conditional distribution of $X|Z=z$, $F_{Y|Z}(\cdot|z)$ be the conditional distribution of $Y|Z=z$, $F_{XY|Z}(\cdot|z)$ be the conditional distribution of $(X, Y)|Z = z$, and $\mathcal{Z}$ be the support of $Z$. Assume that $F_{X|Z}(\cdot|z)$ and $F_{X|Z}(\cdot|z)$ are continuous in $x$ and $y$ for all $z \in \mathcal{Z}$. Then there exists a unique conditional copula $C(\cdot|z)$ such that

	\begin{eqnarray}
		F_{XY|Z}\left(x,y|z\right) & = & C\left(F_{X}\left(x|z\right), F_{Y}\left(y|z\right)|\,z\right)  \label{eq:cond_sklars} \\
		 & &\forall(x, y) \in \bar{\mathbb{R}} \times \bar{\mathbb{R}} \text{ and each } z \in \mathcal{Z} \nonumber
	\end{eqnarray}

	Conversely, if we let $F_{X|Z}(\cdot|z)$ be the conditional distribution of $X|Z=z$, $F_{Y|Z}(\cdot|z)$ be the conditional distribution of $Y|Z=z$, and {$C(\cdot|w)$} be a family of conditional copulas that is measurable in z, then the function $F_{XY|Z}(\cdot|z)$ defined by equation (\ref{eq:cond_sklars}) is a conditional bivariate distribution function with conditional marginal distributions $F_{X|Z}(\cdot|z)$ and $F_{Y|Z}(\cdot|z)$.  $_{\blacksquare}$
\end{thm}

\cite{Patton_2006} stresses that the same conditioning variables must be used for each margin and the copula. Using different conditioning variables for the marginal models and the copula may yield a function $\hat{F}_{XY}\left(\cdot|Z\right)$ that is not the joint distribution of $\left(X,Y\right)|Z$. In practice, this complication has the potential to offset the most attractive quality of the copula approach---modeling the marginal distributions and the copula separately. There is a way to salvage the situation, though. Say there are two conditioning variables, $Z_{1}, Z_{2} \in Z$, and we condition $X$ on $Z_{1}$ and $Y$ on $Z_{2}$. The only way $\hat{F}_{XY}\left(\cdot|Z\right)$ will be the joint distribution function of $\left(X,Y\right)|Z$ is the special case when $F_{X}\left(x|Z_{1}\right) = F_{X}\left(x|Z_{1},Z_{2}\right)$ and $F_{Y}\left(y|Z_{2}\right) = F_{Y}\left(y|Z_{1},Z_{2}\right)$. This can occur when $Z_{2}$ is orthogonal to $Y|Z_{1}$ and $Z_{1}$ is orthogonal to $X|Z_{2}$. In this essay, tests for these orthogonality conditions are included in the diagnostics checks discussed at the end of Section \ref{sec:marginal_distributions}.


\subsection{Methods for Constructing Copulas}

There are several methods for constructing copulas. The three most popular methods are used in this study and subsequently discussed.

\subsubsection{The Inversion Method}

The Gaussian and Student-t Copula are created using the inversion method: $C\left(u,w\right) = F_{XY}\left(F_{X}^{-1}\left(u\right),F_{Y}^{-1}\left(w\right)\right)$, where $F_{X}^{-1}$ and $F_{Y}^{-1}$ are quantile functions. A more exact description of the bivariate Student-t copula distribution is written as:

\begin{eqnarray} \label{eq:stdTinv}
	C_{\boldsymbol{t}}\left(u,w;\rho,\nu\right) &=& \boldsymbol{t}_{v}\left(t_{\nu}^{-1}\left(u\right), t_{\nu}^{-1}\left(w\right)\right) \\
	&=& \int_{-\infty}^{t_{\nu}^{-1}\left(u\right)}\int_{-\infty}^{t_{\nu}^{-1}\left(w\right)}q\left(1-\rho\right)^{-\frac{1}{2}}\left[1+\nu^{-1}\left(x^{2}-2\rho xy + y^{2}\right)\right]^{-\left(\nu + 2\right)
	/2}dxdy \notag
\end{eqnarray}

where $\boldsymbol{t}_{v}$ is the bivariate t-distribution function, $t_{\nu}^{-1}$ is the univariate quantile function, $x = t_{\nu}^{-1}\left(u\right)$, $y = t_{\nu }^{-1}\left(w\right)$, and $q = \Gamma\left(\frac{1}{2}\right)^{-1}\Gamma\left(\frac{\nu +2}{2}\right)\left(\nu\pi\right)^{-1}$. Differentiation of equation (\ref{eq:stdTinv}) with respect to $u$ and $w$ produces the Student-t copula density:

\begin{eqnarray}
	c_{\boldsymbol{t}}\left(u,w;\rho,\nu\right) &=& Q\left(1-\rho\right)^{-\frac{1}{2}}\left[1+\nu^{-1}\left(1-\rho\right)^{-1}\left(x^{2}-2\rho xy+y^{2}\right)\right]^{-\left(\nu +2\right)/2} \notag \\
	&& \times~\left[\left(1+\nu^{-1}x^{2}\right)\left(1+\nu^{-1}y^{2}\right)\right]^{\left(v+1\right)/2}
\end{eqnarray}

where $Q=\Gamma\left(\frac{v}{2}\right)^{-1}\Gamma\left(\frac{\nu +1}{2}\right)^{-2}\Gamma\left(\frac{\nu +2}{2}\right)$. The Gaussian copula is obtained in a similar fashion. If $\boldsymbol{N}_{\rho}$ denotes the standard bivariate normal distribution and $\Phi^{-1}$ denotes the quantile function for the univariate standard normal distribution, the Gaussian copula is summarized by $C_{\boldsymbol{N}}\left(u,w;\rho\right) = \boldsymbol{N}_{\rho}\left(\Phi^{-1}\left(u\right),\Phi^{-1}\left(w\right)\right)$.

\subsubsection{Archimedean Copulas}

The second class we consider are referred to as Archimedean copulas. Families of these copulas are defined by their generator function $\varphi$, which usually contain one or two parameters. These parameters play a fundamental role in specifying the dependence structure between the marginal distributions. Consider the following definitions:

\begin{defn} \label{def:Arch1}
	The generator $\varphi$ of an Archimedean copula is a function from $\left[0,1\right]$ to $\left[0,\infty\right]$ that is continuous, strictly decreasing, convex, and satisfies $\varphi\left(1\right) = 0$. $_{\blacksquare}$
\end{defn}

\begin{defn} \label{def:Arch2}
	The pseudo-inverse of $\varphi$ is the function $\varphi^{\left[-1\right]}$ from $\left[0,\infty\right]$ to $\left[0,1\right]$ given by $\varphi^{\left[-1\right]}\left(t\right) = \varphi^{\left[-1\right]}\left(t\right)$ for $t$ in $\left[0,\varphi\left(0\right)\right]$ and $\varphi^{\left[-1\right]}\left(t\right) = 0$ for $t$ in $\left[\varphi\left(0\right),\infty\right]$. If $\varphi\left(0\right) = \infty$, the pseudo-inverse is simply the inverse $\varphi^{-1}\left(t\right)$. $_{\blacksquare}$
\end{defn}

Given definition~\ref{def:Arch1} and definition~\ref{def:Arch2}, an Archimedean copula is constructed in the following manner:

\begin{equation} \label{eqn:Archm}
	C\left(u,w\right) = \varphi^{\left[-1\right]}\left(\varphi\left(u\right)+\varphi\left(w\right)\right)
\end{equation}

Again, when viewed through the context of Theorem~\ref{thm:skl}, one can interpret $C\left(u,w\right)$ as a cumulative distribution function. To provide a concrete example, consider the Gumbel copula. The generator function is given by $\varphi\left(t\right) = \left(-\ln t\right)^{\theta}$. Using equation (\ref{eqn:Archm}), it is straight forward to construct the copula that corresponds to this particular generator:

\begin{equation} \label{eqn:Gumbel}
	C_{G}\left(u,w;\theta\right) = \exp\left\{-\left[\left(-\ln u\right)^{\theta}+\left( -\ln w\right)^{\theta }\right]^{\frac{1}{\theta}}\right\}
\end{equation}

Differentiating equation~\ref{eqn:Gumbel} with respect to $u$ and $w$ yields the copula density:

\begin{eqnarray}
	c_{G}\left(u,w;\theta\right) &=& C_{G}\left(u,w;\theta\right)\left(\ln u\cdot\ln w\right)^{\theta -1}\left(uw\right)^{-1}\left[\left(-\ln u\right)^{\theta}+\left(-\ln w\right)^{\theta}\right]^{\frac{1-2\theta}{\theta}}\times  \notag \\
	&& \left[\theta -1+\left[\left(-\ln u\right)^{\theta}+\left(-\ln w\right)^{\theta}\right]^{\frac{1}{\theta}}\right]
\end{eqnarray}

\subsubsection{Mixture Copulas} \label{sec:mixtureCopula}

A third method for copula construction exists if one takes an amalgam of copulas and combines them in a certain manner. Just as we can create new distributions by taking a mixture of Gaussians, we can construct mixture copulas. Specifically, if we have a finite collection of $N$ copulas, we can show that the copula $C^{\ast}$ in equation \ref{eqn:mixture} satisfies definition \ref{defn:copula}.

\begin{equation} \label{eqn:mixture}
    C^{\ast} \left(u, w; \boldsymbol{\theta} \right) = \sum_{k=1}^{N} \pi_{k} C_{k} \left(u, w; \theta _{k} \right)
\end{equation}

with $\pi _{k} \geq 0; \pi_{1} + \cdots + \pi_{N} = 1$ and $\boldsymbol{\theta} = \left(\theta_{1}, ..., \theta_{N}, \pi_{1}, ..., \pi_{N} \right)$.

The mixture concept can be extended to include a collection of infinite copulas. Suppose we have a copula family governed by a single parameter $C\left(\cdot;\theta\right)$. If we consider the parameter $\theta$ as random variable drawn from a continuous distribution $\theta\sim H$, then integrating over the support of $H$ produces a copula:

\begin{equation}
	C^{\ast}\left( u,w\right) =\int_{\mathbb{R}}C\left(u,w;\theta\right)dH\left(\theta\right)
\end{equation}

The distribution $H$ is referred to as the 'mixing distribution'. Notice that in the infinite case (2.10), the copula family is consistent while the parameter of that family varies. This is in contrast to the finite mixture case~\ref{eqn:mixture}, where the component copulas $C_{k}$ are free to differ from one another.


\subsection{Survival Copulas and Symmetry}

It was mentioned that Theorem~\ref{thm:skl} confirms the existence of a copula $C$ whose value $C\left(u,w\right) = C\left(F_{X}\left(x\right), F_{Y}\left(y\right)\right)$ provides the probability $P\left[X < x, Y < y\right] = P\left[U < u, W < w\right]$. At times, the joint probability $P\left[X > x, Y > y\right] = 1-F_{XY}\left(x,y\right)\equiv S_{XY}\left(x,y\right)$ is also of interest. As econometricians, we refer to the function $S_{XY}$ as a survival or duration function. For an arbitrary copula $C$, there is a corresponding survival copula $\hat{C}$ that models the joint survival function:

\begin{eqnarray*}
S_{XY}\left(x,y\right) &=& P\left[X > x, Y > y\right] \\
						&=& 1-F_{X}\left(x\right) - F_{Y}\left(y\right) + F_{XY}\left(x,y\right) \\
						&=& S_{X}\left(x\right) + S_{Y}\left(y\right) - 1 + C\left(1 - S_{X}\left(x\right), 1 - S_{Y}\left(y\right)\right) \\
						&=& \hat{C}\left(S_{X}\left(x\right), S_{Y}\left(y\right)\right)
\end{eqnarray*}

The relationship between a copula $C$ and its survival copula $\hat{C}$ is given by:

\begin{equation} \label{eqn:survival}
	\hat{C}\left( u,w\right) =u+w-1+C\left( 1-u,1-v\right)
\end{equation}

Equation (\ref{eqn:survival}) has the effect of rotating a copula 180 degrees along the main diagonal of the unit square. Many papers in the literature discuss the process of producing \textit{symmetric} copulas, especially in the bivariate case. There is nothing intimidating about this process. It is a special case of the mixture method defined in Section \ref{sec:mixtureCopula}. For $N=2$, one can take an arbitrary copula $C$, its corresponding survival copula $\hat{C}$, and apply equal weights so that $C^{\ast}\left(u,w\right) = 0.5\left[ C\left(u,w\right) +\hat{C}\left(u,w\right)\right]$. At times, restricting the weights in mixture copula can aid in identification. If the model is simple enough, this restriction can be jettisoned, and the weights can be estimated along with the parameters of the component copulas.

\subsection{Dependence}

There are many forms of dependence when discussing multivariate distributions. This section outlines two specific forms: rank correlation described by Kendall's tau and 'tail--dependence'. Noticeably, our analysis avoids using Pearson's correlation coefficient $\rho = cov\left(X,Y\right)\left[var\left(X\right)var\left(Y\right)\right]^{-1/2}$, which provides a measure of \textit{linear} association. Although appropriate for elliptical distributions, it is less appropriate for discussing the association of random variables whose joint distribution is non-elliptical, or whose dependence is non-linear. A practical discussion of this issue is covered in \cite{Embrechts_McNeil_Straumann_2002}.

\subsubsection{Kendall's Tau}

A global measure of association, Kendall's tau is a popular measure of the degree of monotonic dependence between two random variates. Invariant to strictly increasing transformations, Kendall's tau provides a measure of correlation between the ranks of two marginal distributions. This short section begins with the definition of concordant and discordant observations.

\begin{defn}
	For a vector of continuous random variables $\left(X,Y\right)$, let $\left(x_{i},y_{i}\right)$ and $\left(x_{j},y_{j}\right)$ represent two observations. We say $\left(x_{i},y_{i}\right)$ and $\left(x_{j},y_{j}\right)$ are concordant if $\left(x_{i} - x_{j}\right)\left(y_{i}-y_{j}\right) > 0$ and discordant if $\left(x_{i}-x_{j}\right)\left(y_{i} - y_{j}\right) < 0$. $_{\blacksquare}$
\end{defn}

The population version of Kendall's tau is defined as the probability of concordance minus the probability of discordance:

\begin{equation}
	\tau = P\left[\left(X_{1} - X_{2}\right)\left(Y_{1} - Y_{2}\right) > 0\right] - P\left[\left(X_{1} - X_{2}\right)\left(Y_{1} - Y_{2}\right) < 0\right]
\end{equation}

For an arbitrary copula $C$, Nelson (2006, page 159) demonstrates that Kendall's tau can be obtained by integrating over the unit square in the following manner:

\begin{equation} \label{eqn:Ktau}
	\tau = 4\int\int_{\left[0,1\right]^{2}}C\left(u,w\right)dC\left(u,w\right) - 1 
\end{equation}

By choosing to represent global dependence through Kendall's tau, we are creating an appropriate standard to compare estimates of dependence produced by different copulas, elliptical or otherwise. Equation (\ref{eqn:Ktau}) is especially helpful when calculating Kendall's tau for mixture copulas, where in most cases, no closed form relationship exists between the parameters of the mixture copula and the measure of correlation. Brute force numerical integration of (\ref{eqn:Ktau}) is a useful fallback in these situations.

\subsubsection{Tail Dependence}

Tail dependence is a localized measure of association and allows us to make probabilistic statements about observations in the upper-right corner and lower-left corner of the unit square. In our present context on exchange rates, upper tail dependence is a statement about the probability of two currency pairs experiencing strong concurrent depreciations against the dollar. Conversely, lower tail dependence is a statement about the probability of two currency pairs experiencing strong concurrent appreciations against the dollar. It is worth pointing out that tail dependence is a property of the copula and not of the marginal distributions. The following definition is due to \cite{Joe_1997}.

\begin{defn} \label{defn:tail_dep}
	For a copula $C$, if $\lambda^{U}\equiv\lim_{t\nearrow 1}P\left[X > F_{X}^{-1}\left(t\right) | Y > F_{Y}^{-1}\left(t\right)\right] = \lim_{t\nearrow 1}\frac{1-2t+C\left(t,t\right)}{1-t}$ exists, then $C$ has upper tail dependence if $\lambda^{U}$ is in $(0,1]$ and no upper tail dependence if $\lambda^{U}=0$. Likewise, if $\lambda ^{L}\equiv \lim_{t\searrow 0}P\left[X\leq F_{X}^{-1}\left(t\right) | Y\leq F_{Y}^{-1}\left(t\right)\right] =\lim_{t\searrow 0}\frac{C\left(t,t\right)}{t}$ exists, $C$ has lower tail dependence if $\lambda ^{L}$ is in $(0,1]$ and no lower tail dependence if $\lambda^{L} = 0$. $_{\blacksquare}$
\end{defn}

When considering a mixture copula $C^{\ast}$ as described in equation (\ref{eqn:mixture}), the tail dependence is the weighted average of the tail dependence values for each component copula:

\begin{equation} \label{eqn:mixtaildep}
\lambda^{\ast, i} = \sum_{k=1}^{N}\pi _{k}\lambda_{k}^{i}
\begin{array}{c}
~~~~~~~i = \left\{L,U\right\} 
\end{array}
\end{equation}

The weights $\pi_{k}$ in equation (\ref{eqn:mixtaildep}) are the same as the corresponding mixture weights in equation (\ref{eqn:mixture}). This result can be verified by applying l'H\^{o}pital's rule to the limits in definition (\ref{defn:tail_dep}), substituting in $dC^{\ast}$, and simplifying.

 \subsection{Estimation}

Maximum likelihood is the most frequently used procedure for parametric estimation of copula models and is the method we employ here. A joint distribution is described by

\begin{equation} \label{eqn:copulaJointDistribution}
	F_{XY}\left(x,y\right) = C\left(F_{X}\left(x\right), F_{Y}\left(y\right)\right)
\end{equation}

Provided that $F_{X}$ and $F_{Y}$ are differentiable while $F_{XY}$ and $C$ are twice differentiable, the density function is found by taking the cross partial derivative of (\ref{eqn:copulaJointDistribution}):

\begin{eqnarray*}
f_{XY}\left(x,y\right) &\equiv& \frac{\partial^{2}F_{XY}\left(x,y\right)}{\partial x \partial y} \\
    &=& \frac{\partial F_{X}\left(x\right)}{\partial x} \cdot \frac{\partial F_{Y}\left(y\right)}{\partial y} \cdot \frac{\partial^{2}C\left(F_{X}\left(x\right), F_{Y}\left(y\right)\right)}{\partial x\partial y} \\
    &=& f_{X}\left(x\right) \cdot f_{Y}\left(y\right) \cdot c\left(u,w\right)
\end{eqnarray*}

The log-likelihood value follows immediately.

\begin{eqnarray*}
\Lagr &=& \sum\nolimits_{t=1}^{T}\log \left[f_{X}\left(x_{t}\right)\right] + \sum\nolimits_{t=1}^{T}\log \left[f_{Y}\left(y_{t}\right)\right]
+ \sum\nolimits_{t=1}^{T}\log \left[c\left(u_{t}, w_{t}\right)\right] \\
    &=& \Lagr_{x} + \Lagr_{y} + \Lagr_{c}
\end{eqnarray*}

Full information maximum likelihood would require the simultaneous estimation of both the marginal distribution parameters along with the parameters of the copula. Although this approach would provide the most efficient estimates, the large dimensionality of the parameter space makes maximization of the likelihood function particularly difficult. A second, more feasible approach known as the 'inference on margins' (IFM) first estimates the marginal models and then separately estimates the copula model. This method leads to less efficient, but still consistent estimates.

\section{Time-Varying Models} \label{sec:time_varying_models}

A brief survey of various time-varying copulas in the literature is provided by \cite{Manner_and_Reznikova_2012}. All models are generalized to $k$ different regimes.

\subsection{Sequential Break Point Analysis}

This first method we use to introduce time-varying dependence is the most simple. \cite{Dias_and_Embrechts_2004} and \cite{Dias_and_Embrechts_2009} outline a framework to detect structural breaks in the parameters of a copula over time. That framework is employed here. Consider the random vectors $\left(\boldsymbol{u}, \boldsymbol{w}\right)$ created by the PIT from the marginal models where $u_{t}\equiv F_{X}\left(x_{t}|Z\right)$ and $u_{t}\equiv F_{Y}\left(y_{t} | Z\right)$. The dependence at each date can be modeled by a copula $C\left(u_{t}, v_{t};~\boldsymbol{\theta}_{t}\right)$ with parameter vector $\boldsymbol{\theta}_{t}=(\theta_{1,t},...,\theta_{N,t})$. The vector $\boldsymbol{\theta}_{t}$ should contain the weights $\pi _{i,t}$ for any mixture copula described in Section \ref{sec:mixtureCopula}. The idea is to test the null hypothesis of an absence of any structural break in the copula parameters

\begin{equation}
	H_{0}:\boldsymbol{\theta}_{t} = \boldsymbol{\theta}
\end{equation}

against the alternative hypothesis of a single structural break at unknown time $\tau \in \left[1, T\right]$.

\begin{equation}
H_{A}:\boldsymbol{\theta}_{t} = \left\{
\begin{array}{ccc}
    \boldsymbol{\theta}_{1} & \text{if} & 1\leq t \leq \tau \\ 
    \boldsymbol{\theta}_{2} & \text{if} & \tau < t \leq T
\end{array}
\right. 
\end{equation}

Define $L_{0}\left(\theta_{0}\right)$, $L_{1}\left(\theta_{1}\right)$, and $L_{2}\left(\theta_{2}\right)$ as the log-likelihood functions of the copula model estimated on the entire sample, the first $\tau$ observations, and the last $T-\tau$ observations, respectively. The statistic $\Lambda_{\tau}$ for a structural break in the parameter of the copula model takes the form of a generalized likelihood ratio test:

\begin{equation}
	-2\log \left(\Lambda_{\tau}\right) = 2\left[L_{1}\left(\hat{\theta}_{1}\right) + L_{2}\left(\hat{\theta}_{2}\right) - L_{0}\left(\hat{\theta}_{0}\right)\right]
\end{equation}

Since the break point $\tau$ is not known, ex ante, a sequence of tests will be performed for each date in a subset of the full sample $\left(t_{l},t_{h}\right) \subset \left[1, T\right]$. To avoid the decreasing power of the test as the break point nears the edge of the series, the subset of potential break dates $\left( t_{l},t_{h}\right)$ will exclude a number of dates at the beginning and end of the sample series. A five percent trim is used at both ends. With this setup, the null will be rejected for large values of:

\begin{equation}
	\zeta_{t}=\max_{t_{l}\leq t\leq t_{h}}\left[ -2\log \left( \Lambda _{t}\right)\right]
\end{equation}

Inference is based on the asymptotic distribution of $\zeta_{t}^{1/2}$, which has been derived by \cite{Csorgo_and_Horvath_1997}. Due to the slow convergence of $\zeta_{t}^{1/2}$ to its asymptotic distribution, an approximation is used to improve the rejection regions of the test for small samples. Simulation results in \cite{Dias_and_Embrechts_2004} provided tentative evidence that this approximation provides accurate p-values.

\begin{eqnarray}
P\left(\zeta_{t}^{1/2} \geq x\right) &\approx& \frac{x^{p} \exp\left\{-x^{2}/2\right\}}{2^{p/2}\Gamma \left(p/2\right)} \times  \\
                                 && \left[\log \frac{\left(1-h\right) \left(1-l\right)}{hl} - \frac{p}{x^{2}}\log \frac{\left(1-h\right)\left(1-l\right)}{hl} + \frac{4}{x^{2}} + O \left(\frac{1}{x^{4}}\right)\right]
\end{eqnarray}

Taking into consideration the possibility of more than one break, the approach described in this section can be iterated on smaller disjoint subsets produced by successful rejections of the null. Say we find an estimate $\hat{\tau}$ of a break date in our full series. We can test for additional breaks in the subsamples $\left[1, \hat{\tau}\right]$ and $\left[\hat{\tau} + 1, T\right]$. This process can continue until the null hypothesis is no longer rejected for any subsample.

Once a collection of break dates $\hat{\tau}_{1},\hat{\tau}_{2},...,\hat{\tau}_{k-1}$ has been estimated, \cite{Dias_and_Embrechts_2009} suggest a repartition method identical to \cite{Bai_1997}. Since the alternative hypothesis of the test is that of a single break, the sequential process to find additional breaks could lead to biased estimates for the location of $\hat{\tau}_{i}:i > 1$. To alleviate these concerns, each break date is reestimated $\hat{\tau}_{i}^{\ast}$ by applying the test to the subsamples $\left[ \hat{\tau}_{i-1}+1\leq t\leq \hat{\tau}_{i+1}\right]$ for $i=1,...,k-1$. Here $\hat{\tau}_{0} = 0$ and $\hat{\tau}_{k} = T$.

The advantages of this approach is its relative simplicity. Many static copula have tractable gradients and hessians. The dimensionality of the parameter space is kept to a minimum as well. The only dynamics, so-to-speak, are the estimated break points. Other regime-varying models in the literature, such as the Markov Transition and Smooth Transition models discussed later, usually produce results where the dependence structure of the underlying data may change from day-to-day.

\subsection{Markov Transition}

In a second approach, this essay uses a latent state variable $s_{t}$ to govern the dependence of our exchange rate series at time $t$. First brought into the econometrics literature by \cite{Hamilton_1989} and \cite{Hamilton_1994}, changes in the state variable are modeled by a first-order markov transition matrix. In this setup, the probability of being in some particular state tomorrow $s_{t+1}$ depends solely on what state the variable is in today $s_{t}$. A more concrete description of the process is given by the probability statement $P\left[s_{t + 1} = j~|~s_{t}=i,~s_{t-1} = m,\ldots \right] = P\left[s_{t+1}=j~|~s_{t} = i\right] = p_{ij}$, where we have $p_{i1} + p_{i2} + \cdots + p_{ik}=1$ for all $i = 1,...,k$. The vectors of transition probabilities from one state to another are organized into a single $k\times k$ transition matrix $\mathbf{P}$.

\begin{equation}
\mathbf{P}=\left[ 
\begin{array}{cccc}
p_{11} & p_{21} & \cdots & p_{k1} \\ 
p_{12} & p_{22} & \cdots & p_{k2} \\ 
\vdots & \vdots & \ddots & \vdots \\ 
p_{1k} & p_{2k} & \cdots & p_{kk}
\end{array}
\right]
\end{equation}

For each of the $k$ possible states, a copula is used to model the dependence for that state. These copula densities are stacked into a $k\times 1$ vector and indexed by time:

\begin{equation}
\boldsymbol{\eta}_{t} = \left[ 
\begin{array}{c}
\eta _{t,1} \\ 
\vdots \\ 
\eta _{t,k}
\end{array}
\right] = \left[
\begin{array}{c}
c\left(F_{X}\left(x_{t} | \boldsymbol{z}_{t}\right), F_{Y}\left(y_{t} | \boldsymbol{z}_{t}\right) | \boldsymbol{z}_{t} ;\mathbf{\theta}_{1}\right) \\ 
\vdots \\ 
c\left(F_{X}\left(x_{t} | \boldsymbol{z}_{t}\right), F_{Y}\left(y_{t} | \boldsymbol{z}_{t}\right) | \boldsymbol{z}_{t} ;\mathbf{\theta }_{k}\right)
\end{array}
\right]
\end{equation}

Since the current state of the process is unknown at any given date, inference must be made. At each date, the probability that the dependence governing the exchange rate returns comes from state $j$ is defined as $P\left[s_{t}=j~|~x_{t},x_{t-1},...,y_{t},y_{t-1},...,\mathbf{\theta}_{1},...,\mathbf{\theta}_{k}\right]$. These conditional probabilities are collected into another $k\times 1$ vector denoted $\boldXi_{t|t}$. Given a set of initial probabilities $\boldXi_{1|0}$, optimal inferences are found by iterating on the following equations:

\begin{eqnarray}
	\boldXi_{t|t} &=& \frac{\boldXi_{t|t-1}\odot\boldsymbol{\eta}_{t}}{\mathbf{1}^{\top}\left(\boldXi_{t|t-1}\odot \boldsymbol{\eta}_{t}\right)} \\
	\boldXi_{t+1|t} &=& \mathbf{P} \boldXi_{t|t}
\end{eqnarray}

where the operator $\odot$ is the hadamard product denoting element-by-element multiplication and $\mathbf{1}$ is a column of ones of length $k$. Conveniently, the log-likelihood value can be calculated as a side product to this algorithm, with the value equaling

\begin{equation}
\Lagr_{c} = \sum_{t=1}^{T}\log \left[\mathbf{1}^{\top}\left(\boldXi_{t|t-1} \odot \boldsymbol{\eta}_{t}\right)\right] 
\end{equation}

What is more, we can use these conditional probabilities to gain an even better gauge of the latent state variable. Starting with $\boldXi_{T|T}$, we can iterate backwards from $T$ to $1$ to calculate the smoothed probabilities $\boldXi_{t|T}$ by iterating on:

\begin{equation} \label{eqn:Kim_smooth_probability}
	\boldXi_{t|T} = \boldXi_{t|t} \odot \left[\mathbf{P}^{\top}\left(\boldXi_{t+1|T}\left(\div\right)\boldXi_{t+1|t}\right)\right]
\end{equation}

The operator $\left(\div\right)$ denotes element-by-element division. Changes in dependence through time will be captured by the different copula parameters governing each state.

\subsection{Smooth Transition}

The third, and final, approach to time-varying dependence this study implements is a version of the smooth-transition copula-GARCH (STCG) model. We employ a single copula but allow the parameters of the copula to evolve over time. More specifically, for any given copula $C\left(\cdot | \mathbf{\theta}_{t}\right)$, we allow the possibility for each parameter $\boldsymbol{\theta}_{i}$ to undergo multiple transitions.

\begin{equation} \label{eqn:smoothTrans}
	\theta_{i,t} = \theta_{i,1} + \sum_{m=1}^{k-1}\left(\theta_{i,m+1} - \theta_{i,m}\right) G_{m}\left(s_{t}; c_{i,m},\gamma_{i,m}\right)
\end{equation}

In general, $G_{m}\left(\cdot\right)$ is a bounded function with respect to the continuous transition variable $s_{t}$. In this study, $s_{t}$ will be constructed as a linear time trend, $s_{t}=\frac{t}{T}$, and $G_{m}\left(\cdot\right)$ takes the form of the logistic function:

\begin{equation} \label{eqn:smoothTransLogit}
	G\left(s_{t}; c, \gamma\right) = \left(1+\exp\left\{ -\gamma \left(s_{t}-c\right) \right\}\right)^{-1}~~~~~~~\gamma > 0
\end{equation}

Other specifications for $G\left( \cdot \right) $ can be found in \cite{Dijk_and_Frances_1999} and \cite{Ocal_and_Osborn_2000}. The parameters $\boldsymbol{\gamma}_{i} = \left(\gamma_{i,1},...,\gamma_{i,k-1}\right)$ control the pace of each transition while the members of vector $\mathbf{c}_{i} = \left(c_{i,1},...,c_{i,k-1}\right)$ locate the inflection points of each transition. It is difficult to overstate how nonlinear a copula model becomes when subject to the full generality of specifications (\ref{eqn:smoothTrans}) and (\ref{eqn:smoothTransLogit}). To aid in identification, some restrictions are usually imposed. In our case, we restrict the location variables of each transition to be identical across copula parameters: $c_{i,m}=c_{j,m}$ for all $i,j$ and each $m$. In addition, we also hold the weights $\pi_{i}$ for mixture copulas constant.

\section{Marginal Distributions} \label{sec:marginal_distributions}

As stated earlier, the copula approach enables us to model the marginal distributions and dependence structure separately. In this section, we summarize how each univariate margin is modeled and validated. For any particular exchange rate, define the return as $x_{t} = \log \left(S_{t}\right) - \log\left(S_{t-1}\right)$, where $S_{t}$ is the spot price at time $t$. All exchange rates are denoted in foreign currency units per US dollar. Like many other financial assets, exchange rates are subject to volatility clustering, skewness, and leptokurtosis. To manage these properties, each return series $\left\{x_{t}\right\}_{t=1}^{T}$ is modeled using a ARMA-GARCH specification with a Student-t distribution or a skewed Student-t distribution. The overall dynamics of the ARMA-GARCH process can be summarized by the following two equations.

\begin{equation} \label{eqn:marginalModel}
    x_{t} = \mu_{t} + \varepsilon_{t}
\end{equation}

\begin{equation}
	\varepsilon_{t} = \eta_{t} \sqrt{h_{t}}
\end{equation}

where $\eta_{t}$ are independent and identically distributed with $E[\eta_{t}] = 0$ and $E[\eta^{2}_{t}] = 1$. and has a probability distribution $g$. The conditional mean for the log-returns can be formulated by the general ARMA process:

\begin{equation} \label{eq:conditional_mean}
    \mu_{t} = \phi_{0} + \sum_{i=1}^{p} \phi_{i} x_{t-i} + \sum_{j=1}^{q} \theta_{j} \, \varepsilon_{t - j}
\end{equation}

 The value $p$ is the order of of autoregressive component and $q$ is the order of the moving average component. The conditional volatility process for the models under consideration can be generalized with the following formula. Functions \emph{A} and \emph{B} are generic stand-ins that will differ across the various GARCH specifications.

\begin{equation} \label{eq:conditional_var}
    h_{t} = \omega + \sum_{j=1}^{b} \alpha_{j} \emph{A}(\varepsilon^{2}_{t - j}) + \sum_{i=1}^{k} \beta_{i} \emph{B}(h_{t - i})
\end{equation}

The conditional mean and variance are used to center and scale the innovation terms.

\begin{equation} \label{eq:z_score_returns}
    z_{t} = \frac{x_{t} - \mu_{t}}{\sqrt{h_{t}}}
\end{equation}

By collecting all the parameters in the conditional mean and variance equations into one vector $\Delta = [\phi, \theta, \alpha, \beta]$, the distribution for the log-returns $f$ can be written as a function of $\Delta$, the chosen error distribution $g$, the conditional mean $\mu_{t}$ and conditional variance $\sigma^{2}$, and any necessary shape parameters $\lambda$ of the distribution:

\begin{equation} \label{eq:conditional_distr}
    f(x_{t} | \mu_{t}, \sigma^{2}_{t}, \lambda, \Delta) = \frac{1}{\sqrt{h_{t}}} g(z_{t} | \lambda, \Delta)
\end{equation}

In certain times the researcher may want the capability to accurately model return values that are not perfectly symmetric which is a characteristic of the t-distribution. To produce a skewed t-distribution, this essay follows \cite{Fernandez_and_Steel_1998}. For a unimodal and symmetric density $f\left(x\right)$ that is decreasing in $\left\vert x\right\vert$, a skew parameter $\xi$ can force asymmetry upon $f\left(x\right)$, generating a new distribution $p\left(x|\xi\right)$ where

\begin{equation}
	p\left(x|\xi \right) = \frac{2}{\xi + \frac{1}{\xi}} \left[f\left(x\xi\right) \mathbf{I}_{\left\{x < 0\right\}} + f\left(x\xi^{-1}\right) \mathbf{I}_{\left\{x \geq 0\right\}}\right]
\end{equation}


\subsection{Conditional Variance}

For the conditional variance, two different specifications are employed in this essay. The first is the standard GARCH(b,k) model,which has the following form:

\begin{equation} \label{eqn:GARCH}
	h_{t} = \omega + \sum_{i = 1}^{b}\alpha_{i} \varepsilon_{t - i}^{2} + \sum_{j = 1}^{k}\beta_{j} h_{t - j}
\end{equation}

A separate approach proposed by \cite{GJR_1993} adds a bit of flexibility to the GARCH model specified in equation (\ref{eqn:GARCH}) by allowing asymmetrical responses to shocks\footnote{By shocks, we are referring to the error term in equation (\ref{eqn:marginalModel})} of opposing signs. 

\begin{equation} \label{eq:gjrGARCH_definition}
	h_{t} = \omega + \sum_{i = 1}^{b}\left(\alpha_{i} + \gamma_{i}\mathbf{I}_{\left\{\varepsilon < 0 \right\}}\right) \varepsilon_{t - i}^{2} + \sum_{j = 1}^{k}\beta_{j}h_{t- j}
\end{equation}

Here, $\mathbf{I}_{\left\{ \cdot \right\}}$ is the indicator function with value one if the statement in the brackets is true and zero otherwise. This second model allows for asymmetric responses to past shocks $\varepsilon_{t-i}$. If the lagged shock is positive $\left(\varepsilon_{t-i}>0\right)$, the coefficient has the value $\alpha_{i}$. If the lagged shock is negative $\left(\varepsilon_{t-i} < 0 \right)$, the coefficient has the value $\alpha_{i} + \gamma_{i}$.

\subsection{Model Validation}

The aim is to specify a parsimonious model that produces (standardized) residuals that are independent and identically draws from the selected distribution, $g$. To confirm that we have well specified models, a series of diagnostic checks are performed on the fitted ARMA-GARCH model for each currency pair and time-frame. The tests presented for validation are included in the same software that is used to model the log returns as an ARMA-GARCH process, \cite{Rugarch}. There are five categories of tests. One set of tests checks for the correct ARMA specification while a second set of tests checks for the correct ARCH specification. A third test checks the empirical distribution of the standardized residuals against the theoretical distribution selected by the researcher, $g$. A fourth set of tests checks for asymmetric effects in the residuals. This is particular relevant when considering the gjr-GARCH model of equation (\ref{eq:gjrGARCH_definition}). A fifth set of tests checks for parameter stability using \cite{NYBLOM_1989}. 

To test for a correct ARMA specification the weighted Ljung-Box test of \cite{Fisher_Gallagher_2012} is used. The authors propose a statistic based on the determinant of a Toeplitz matrix whose elements are values of a series' sample autocorrelations. The statistic has a functional form that can be viewed as a weighted Ljung-Box statistic.

\begin{equation} \label{eq:wtd_ljung_box_arma_test}
	Q_{W} = n(n + 2) \sum_{j=1}^{m} \frac{m - j + 1}{m} \frac{[r_{j}(x)]^{2}}{n - j}
\end{equation}

The value $m$ is set to the largest lag and the weight term, $(m - j + 1) / m$, decreases as the length of the lag increases. In the equation above, $\hat{r}_{j}(x)$ is used to denote the sample correlation value of the series $x$ at the j$^{th}$ lag.

\begin{equation} \label{eq:sample_correlation}
	r_{j}(x) = \frac{\sum_{t=j+1}^{n} (x_{t} - \bar{x}) (x_{t-j} - \bar{x})}{\sum_{t=1}^{n} (x_{t} - \bar{x})^{2}}
\end{equation}

Two versions of this test are performed: one set checks for any remaining autocorrelation in the residuals of the model, $r_{j}(\hat{\varepsilon}_{t}$), and a second version checks for any remaining autocorrelation in the squared residuals of the model, $r_{j}(\hat{\varepsilon}_{t}^{2}$). Both versions of the test are conducted for $m$ equal to one, three, and five. Under the null hypothesis of a correctly specified model, the statistic $Q_{W}$ is asymptotically distributed as $\sum_{k=1}^{m} \lambda_{k} \chi_{k}^{2}$ where $\chi_{k}^{2}$ are independent chi-squared random variables with one degree of freedom. The weight term, $\lambda_{k}$, is a function of the information matrix of $\phi$ and $\theta$, the parameters of the ARMA model in equation (\ref{eq:conditional_mean}), and the weight term in equation (\ref{eq:wtd_ljung_box_arma_test}), $(m - j + 1) / m$.

In \cite{Fisher_Gallagher_2012}, the authors also provide a test for the correct ARCH specification. The statistic has the following functional form:

\begin{equation}
	L_{W}(b,m) = n \sum_{j=b+1}^{m} \frac{m - j + (b + 1)}{m} [r_{j} \left( \hat{\varepsilon}_{t}^{2} / h_{t} \right)]^{2} 
\end{equation}

Here, $b$ is the ARCH order from equation (\ref{eq:gjrGARCH_definition}). Under the null hypothesis of a correctly specified model, the statistic $L_{W}(b,m)$ is asymptotically distributed as $\sum_{k=1}^{m} w_{k} \chi_{k}^{2}$ where $\chi_{k}^{2}$ are independent chi-squared random variables with one degree of freedom and weight term equal to $(m - j + b + 1) / m$.

To verify the choice of the conditional distribution, $g$, the Goodness-of-Fit test from \cite{VlaadPalm1993} is used to compare the histograms of the standard residuals after they are transformed into their unit interval equivalents. The unit interval is broken into $b$ equal width bins and the empirical frequency of the resulting histogram is checked against the theoretical expectation. The test statistic is distrbuted as $\chi^{2}_{(b-1)}$.

The fourth diagnostic check reported here checks for the presence of any asymmetric effects remaining in the fitted residuals. The Joint Sign-Bias Test of \cite{Engle_Ng_1993} is run with the following regression:

\begin{equation}
	\hat{z}_{t}^{2} = a + b_{1} \mathbf{I}_{\left\{\hat{\varepsilon}_{t} < 0 \right\}} + b_{2} \mathbf{I}_{\left\{\hat{\varepsilon}_{t} < 0 \right\}} \hat{\varepsilon}_{t} + b_{3} \mathbf{I}_{\left\{\hat{\varepsilon}_{t} \geq 0 \right\}} \hat{\varepsilon}_{t} + e_{t}
\end{equation}

The dependent variable, $\hat{z}_{t}^{2}$, is the square of the standardized residuals. The coefficient $b_{1}$ tests for any significant asymmetric effects remaining in the residuals while $b_{2}$ and $b_{3}$ capture the magnitude of the asymmetric effects. The t-ratios of $b_{1}$, $b_{2}$, and $b_{3}$ are the test statistics for the sign-bias, the negative sign-bias, and the positive sign-bias, respectively. The joint test evaluates the significance of the three coefficients together using a LM test whose statistic follows an asymptotic chi-squared distribution with three degrees of freedom. The null hypothesis is for a well specified model with no asymmetric effects present.

Due to a time series' sequential nature, the fitted ARMA-GARCH models are checked for the constancy of their parameters over the duration of the sample. The test used in this essay was proposed by \cite{NYBLOM_1989}. Our approach is to start with a simple GARCH(1,1) model with no mean equation (e.g. no intercept, AR, or MA components). If our tests indicate that the model is well specified, we stop there. If not, we continue to add additional parameters or try different functional forms until an acceptable fit is obtained.

Finally, this essay follows \cite{Patton_2006} to test the orthogonality assumption between the conditional variables of one exchange rate model against the conditional variables of the other marginal models. For two arbitrary exchange rates, A and B, to test for the orthogonality of the conditioning variables in the mean equation of exchange rate B, the residuals of the marginal model for A are regressed against any lagged autoregressive terms in the marginal model for exchange rate B.

\begin{equation} \label{eq:mean_orthogonality_assumption}
	\hat{\varepsilon}_{A, t} = \beta_{0} + \sum_{j=1}^{p_{B}}\beta_{B,j} x_{B, t-j} + \epsilon_{t}
\end{equation}

The p-values for each coefficient in the vector $\boldsymbol{\beta}_{B}$ are reported. Any coefficient that is statistically different than zero is interpreted as a rejection of the orthogonality assumption. The relationship between the two exchange rate are then reversed and the residuals of marginal model B are regressed against any autoregressive terms in the marginal model for A. Tests for the the orthogonality assumption in the variance equation is carried out in a similar manner, but instead of regressing residuals against lagged autoregressive terms, the standardized squared residuals of one marginal model are regressed against the lagged squared residuals from the other models.

\begin{equation}  \label{eq:var_orthogonality_assumption}
	\hat{\varepsilon}_{A,t}^{2} / \hat{h}_{A,t} = \delta_{0} + \sum_{j=1}^{b_{B}}\delta_{B,j} \hat{\varepsilon}^{2}_{B, t-j} + \epsilon_{t}
\end{equation}

The p-values for each coefficient in the vector $\boldsymbol{\delta}_{B}$ are reported. Any coefficient that is statistically different than zero is interpreted as a rejection of the orthogonality assumption.

\subsubsection{Validation Results}

Table \ref{tbl:marginal_model_regressions} summarizes the marginal models of the exchanges rates under review in this paper. This includes the exchange rates between the US Dollar and the following three major global currencies: the European Euro, the Japanese Yen, and the British Pound. A major takeaway from Table \ref{tbl:marginal_model_regressions} is that the GARCH processes for all three exchange rates changes from one estimation period to the next. From 2000-2009, the Euro/USD and Pound/USD exchange rates have asymmetric volatility responses while the Yen/USD exchange rate is symmetric. These specifications flip during the 2010-2018 timeframe. Also, the shape parameters (the degrees of freedom parameter from the t-Distribution) for each exchange rate are collectively lower in 2010-2018 than in 2000-2009, indicating slightly fatter tails for the 2010-2018 timeframe.

Table \ref{tbl:margin_model_diagnostic_pvalues} summarizes the diagnostic checks for the ARMA-GARCH models and shows that, overall, the chosen marginal models are well specified and capture the conditional mean and conditional variance of the underlying processes. The top part of the table records the p-values for the ARCH-LM, ARMA-LM, Goodness-of-Fit, and Sign-Bias tests. The bottom portion of the table records the value of test statistic for Nyblom's test for parameter consistency. One problematic area is the constant parameter assumption for the GARCH intercept term, $\omega$. It is consistently rejected at the 1\% significance level which causes the rejection of the joint parameter test as well. The only other test that rejects the null of a well specified model at the 5\% significance level concerns the 2010-2018 Pound/USD series; the ARMA-LM test on the fifth lag using a degree of 2. Finding a model specification that satisfies this particular test proved insurmountable. A wide range of GARCH models and distributional assumptions were tested but none were able to provide a specification that passed all ARMA-LM tests.

Table \ref{tbl:Conditioning_Variable_Checks} summarizes the orthogonality tests mentioned in \cite{Patton_2006}, which are necessary for the correct interpretation of a conditional copula. One of the tests rejects the null hypothesis of orthogonal conditioning variables. For the 2000-2009 time period, the second lagged ARCH term of the Euro/USD model retains explanatory power for the Yen/USD exchange model, rejecting the null of an orthogonality relationship at the 5\% significance level. This is a cause for concern since it implies that Theorem \ref{thm:cond_skl} may not hold and the Yen/USD and Euro/USD exchange rates may need to be modeled with a bivariate distribution rather than separate univariate models.


\begin{table}
	\fontsize{10pt}{10pt}\selectfont
	\centering
	\caption{Marginal ARMA-GARCH Regressions}
	\begin{tabular}{l l l l | l l l}
		\midrule
				   & \multicolumn{3}{c}{2000-2009} & \multicolumn{3}{c}{2010-2018} \\
		\midrule
				   & Euro & Pound & Yen & Euro & Pound & Yen \\
		\midrule
         $\phi_{0}$    &  -0.00024        & -0.00006     & 0.00013      &    --        &    --        & 0.00012      \\
                       &   0.00011$^{*}$  &  0.00010     & 0.00012      &    --        &    --        & 0.00010      \\ \\
		 $\phi_{1}$    &     --           &     --       &    --        &    --        & 0.00166      &    --        \\
		               &     --           &     --       &    --        &    --        & 0.02098      &    --        \\ \\
         $\omega$      &  0.00000         &  0.00000     & 0.00000      & 0.00000      & 0.00000      & 0.00000      \\
                       &  0.00000$^{**}$  &  0.00000     & 0.00000      & 0.00000      & 0.00000      & 0.00000      \\ \\
         $\alpha_{1}$  &  0.00000         &  0.052       & 0.032        & 0.028        & 0.032        & 0.038        \\
                       &  0.00111         & 0.003$^{**}$ & 0.007$^{**}$ & 0.004$^{**}$ & 0.019        & 0.005$^{**}$ \\ \\
         $\gamma_{1}$  & -0.065           & -0.023       &    --        &    --        &    --        & 0.009        \\
                       &  0.006$^{**}$    & 0.011$^{*}$  &    --        &    --        &    --        & 0.012        \\ \\
         $\alpha_{2}$  &  0.044           &    --        &    --        &    --        & 0.009        &    --        \\
                       &  0.004$^{**}$    &    --        &    --        &    --        & 0.017        &    --        \\ \\
         $\gamma_{2}$  &  0.063           &    --        &    --        &    --        &    --        &    --        \\
                       &  0.004$^{**}$    &    --        &    --        &    --        &    --        &    --        \\ \\
         $\beta_{1}$   &  0.952           & 0.953        & 0.957        & 0.970        & 0.878        & 0.949        \\
                       &  0.000$^{**}$    & 0.006$^{**}$ & 0.007$^{**}$ & 0.003$^{**}$ & 0.005$^{**}$ & 0.005$^{**}$ \\ \\
         $\beta_{2}$   &     --           &    --        &    --        &    --        & 0.074        &    --        \\
                       &     --           &    --        &    --        &    --        & 0.004$^{**}$ &    --        \\ \\
         shape         &  9.387           & 9.953        & 6.879        & 8.388        & 8.301        & 4.907        \\
                       &  1.683$^{**}$    & 1.551$^{**}$ & 1.230$^{**}$ & 0.989$^{**}$ & 0.852$^{**}$ & 0.456$^{**}$ \\ \\
		 Distribution  & T-Dist           & T-Dist       & T-Dist       & T-Dist       & T-Dist       & T-Dist       \\
		 GARCH Spec.   & gjr              &   gjr        &    std       &    std       &    std       &  gjr         \\
		 ARMA          &  0, 0            &  0, 0        &   0, 0       &   0, 0       &   1, 0       &  0, 0        \\
		 GARCH         &  2, 1            &  1, 1        &   1, 1       &   1, 1       &   2, 2       &  1, 1        \\
		\midrule
	\end{tabular}
	\begin{tablenotes}
		\item{\footnotesize \textbf{Signif. Codes:} 0 '**' 0.01 '*' 0.05 '+' 0.1 ' ' 1}
	\end{tablenotes}
	\label{tbl:marginal_model_regressions}
\end{table}


\begin{table}
	\fontsize{10pt}{10pt}\selectfont
	\caption{Exchange Rate Model Diagnostics}
	\begin{tabular}[c]{l l l l l c l l l}
		\midrule
                & & \multicolumn{3}{c}{2000 - 2009} & & \multicolumn{3}{c}{2010 - 2018} \\ \addlinespace[1mm]
		\cline{3-5} \cline{7-9}
    Test  & Spec                             & Euro  & Pound & Yen   & &  Euro & Pound & Yen   \\ \addlinespace[1mm]
		\midrule
  ARCH-LM &        ARCH Lag[1] & 0.417 & 0.930       & 0.109       & & 0.905       & 0.063$^{+}$ & 0.761       \\
  ARCH-LM &        ARCH Lag[3] & 0.813 & 0.785       & 0.262       & & 0.819       & 0.238       & 0.478       \\
  ARCH-LM &        ARCH Lag[5] & 0.900 & 0.837       & 0.460       & & 0.807       & 0.428       & 0.651       \\ \\
  ARMA-LM &    Lag[1] degree=1 & 0.738 & 0.266       & 0.403       & & 0.744       & 0.998       & 0.967       \\
  ARMA-LM &    Lag[2] degree=1 & 0.830 & 0.375       & 0.605       & & 0.847       & 0.297       & 0.663       \\
  ARMA-LM &    Lag[5] degree=1 & 0.832 & 0.489       & 0.875       & & 0.892       & 0.377       & 0.829       \\
  ARMA-LM &    Lag[1] degree=2 & 0.782 & 0.319       & 0.921       & & 0.682       & 0.351       & 0.146       \\
  ARMA-LM &    Lag[2] degree=2 & 0.862 & 0.478       & 0.163       & & 0.382       & 0.489       & 0.241       \\
  ARMA-LM &    Lag[5] degree=2 & 0.879 & 0.756       & 0.078$^{+}$ & & 0.517       & 0.027$^{*}$ & 0.385       \\ \\
      GOF &             20bins & 0.122 & 0.186       & 0.291       & & 0.264       & 0.656       & 0.887       \\
      GOF &             30bins & 0.310 & 0.252       & 0.290       & & 0.295       & 0.312       & 0.441       \\
      GOF &             40bins & 0.327 & 0.425       & 0.260       & & 0.077$^{+}$ & 0.540       & 0.340       \\
      GOF &             50bins & 0.676 & 0.563       & 0.072$^{+}$ & & 0.291       & 0.665       & 0.734       \\ \\
SIGN-BIAS &       Joint Effect & 0.471 & 0.090$^{+}$ & 0.513       & & 0.591       & 0.781       & 0.097$^{+}$ \\
SIGN-BIAS & Negative Sign Bias & 0.376 & 0.161       & 0.647       & & 0.302       & 0.457       & 0.168       \\
SIGN-BIAS & Positive Sign Bias & 0.188 & 0.675       & 0.173       & & 0.532       & 0.540       & 0.562       \\
SIGN-BIAS &          Sign Bias & 0.234 & 0.338       & 0.510       & & 0.487       & 0.700       & 0.319       \\
\midrule
   NYBLOM &         $\phi_{0}$ & 0.161        & 0.276        & 0.118        & &    --        &      --        &   0.258        \\
   NYBLOM &         $\phi_{1}$ &    --        &    --        &    --        & &    --        &   0.251        &      --        \\
   NYBLOM &         $\omega$   & 228.7$^{**}$ & 292.1$^{**}$ & 186.3$^{**}$ & & 293.9$^{**}$ &   232.3$^{**}$ &   134.8$^{**}$ \\
   NYBLOM &       $\alpha_{1}$ & 0.076        & 0.232        & 0.308        & & 0.196        &   0.243        &   0.066        \\
   NYBLOM &       $\gamma_{1}$ & 0.131        & 0.238        &    --        & &    --        &      --        &   0.114        \\
   NYBLOM &       $\alpha_{2}$ & 0.076        &    --        &    --        & &    --        &   0.272        &      --        \\
   NYBLOM &       $\gamma_{2}$ & 0.138        &    --        &    --        & &    --        &      --        &      --        \\
   NYBLOM &        $\beta_{1}$ & 0.093        & 0.228        & 0.198        & & 0.173        &   0.240        &   0.105        \\
   NYBLOM &        $\beta_{2}$ &    --        &    --        &    --        & &    --        &   0.239        &      --        \\
   NYBLOM &              shape & 0.132        & 0.172        & 0.105        & & 0.459$^{+}$  &   0.971$^{**}$ &   0.032        \\
   NYBLOM &         Joint Test & 621.4$^{**}$ & 784.7$^{**}$ & 759.4$^{**}$ & & 598.5$^{**}$ &   687.9$^{**}$ &   524.7$^{**}$ \\		 
\hline \hline
	\end{tabular}
	\begin{tablenotes}
		\item{\footnotesize \textbf{Signif. Codes:} 0 '**' 0.01 '*' 0.05 '+' 0.1 ' ' 1}
		\item{\footnotesize \textbf{Note:} The values in the top part of the table are p-values for the ARCH-LM, ARMA-LM, GOF, and Sign-Bias tests.}
		\item{\footnotesize \textbf{Note:} The bottom part of the table records the value of the test statistic for Nyblom's Test for Constancy of parameters over time. Significant code indicate if the test static exceeds threshold value for a particular significance level. Threshold levels differ between test for a single parameter or a joint test:}
		\item{\footnotesize \textit{Individual statistic}: 10\% = 0.35, 5\%=0.47, 1\%=0.75}
		\item{\footnotesize \textit{     Joint statistic}: 10\% = 1.69, 5\%=1.90, 1\%=2.35.}
		%\item{\footnotesize \textbf{Note:} Goodness-of-fit tests are conducted on the standardized residuals of the marginal models. All figures in the table are p-values. The Kolmogorov-Smirnov (KS) tests checks for a uniform distribution for the standardized residuals with the unit interval as its support. The moment LM tests check the PIT for serial independence. The tests take the form of the regression $\left(u_{t} - \bar{u}\right)^{m} = \sum_{k=1}^{20}\gamma_{k}\left(u_{t-k} - \bar{u}\right)^{m} + \epsilon_{t}$ for $m = 1,...,4$. Under the null of independence, the test statistic $\left(T - 20\right) R^{2}$ for each regression is distributed as $\chi^{2}\left(20\right)$. The likelihood ratio (LR) test checks for joint normality of the mean and variance. The Jarque-Bera (JB) test checks for joint normality of the skew and kurtosis. For all tests, the null hypothesis is a well specified model.}
	\end{tablenotes}
	\label{tbl:margin_model_diagnostic_pvalues}
\end{table}


\begin{table}
	\fontsize{10pt}{10pt}\selectfont
	\centering
	\caption{Orthogonal Assumption Tests}
	\begin{tabular}{l l | l l l | l l l}
		\midrule
				& & \multicolumn{3}{c}{2000-2009} & \multicolumn{3}{c}{2010-2018} \\
		\midrule
				& & Euro & Pound & Yen & Euro & Pound & Yen \\
		\hline
        Mean Equation     & $P_{t-1}$                            & --    & --    & --          &  0.766       & --      & 0.067$^{+}$  \\ \\ \\
		Variance Equation & Euro $\hat{\varepsilon}^{2}_{t-1}$   & --    & 0.760 & 0.138       &  --          & 0.281   & 0.777        \\
                          & Euro $\hat{\varepsilon}^{2}_{t-2}$   & --    & 0.256 & 0.026$^{*}$ &  --          & --      & --           \\
						  & Joint-Test                           & --    & 0.509 & 0.024$^{*}$ &  --          & --      & --           \\ \\
		                  & Pound $\hat{\varepsilon}^{2}_{t-1}$  & 0.381 & --    & 0.189       &  0.173       & --      & 0.724        \\
		                  & Pound $\hat{\varepsilon}^{2}_{t-2}$  & --    & --    & --          &  0.577       & --      & 0.928        \\
						  & Joint-Test                           & --    & --    & --          &  0.373       & --      & 0.939        \\ \\
                          & Yen $\hat{\varepsilon}^{2}_{t-1}$    & 0.130 & 0.248 & --          &  0.099$^{+}$ & 0.346   & --           \\
	\hline
	\end{tabular}
	\begin{tablenotes}
		\item{\footnotesize \textbf{Signif. Codes:} 0 '**' 0.01 '*' 0.05 '+' 0.1 ' ' 1}
		\item{\footnotesize \textbf{Note:} This table checks the orthogonality assumption for the conditional copula. The values in the table are the p-values reported for the coefficients in equations (\ref{eq:mean_orthogonality_assumption}) and (\ref{eq:var_orthogonality_assumption}). For the mean equations, only the 2010-2018 Pound/USD model contains an autoregressive component. Therefore, the residuals of the Euro/USD and Yen/USD are regressed against the first lag of the Pound/USD. For the variance equations all exchange rate models have at least one ARCH component while the 2000-2009 Euro/USD and the 2010-2018 Pound/USD have two ARCH components. The orthogonality test requires regressing the standardized squared residuals of one model against the lagged squared residuals of the other marginal models.}
		\item{\footnotesize \textbf{Note:} It is clear that the orthogonality assumption is violated, at the 5\% significance level for the Euro and Yen exchange rates against the dollar during the 2000-2009 time period. The second lagged square residual is found to have explanatory power for the standard squared residual of the Yen/USD exchange rate. This implies that Theorem \ref{thm:cond_skl} may not hold and the Yen/USD and Euro/USD exchange rates may need to be modeled with a bivariate distribution rather than separate univariate models.}
	\end{tablenotes}
	\label{tbl:Conditioning_Variable_Checks}
\end{table}

\section{Results} \label{sec:Results}

Having modeled the univariate exchange rate series, the uniform residuals from these models can be used as inputs to the three time-varying models described in Section \ref{sec:time_varying_models}. Since the optimal ARMA-GARCH models from Table \ref{tbl:marginal_model_regressions} all utilize the t-distribution, the t-Copula will act as the benchmark copula in this essay. Two alternative copulas will be estimated as well. One is a finite mixture copula like the one described in equation (\ref{eqn:mixture}). This particular mixture combines a regular Gumbel\footnote{The Gumbel copula is an Archimedean copula with generator function $\varphi\left(t\right) = \left(-\ln t\right)^{\theta}$} copula with a survival Gumbel copula. Like in \cite{Patton_2006}, this mixture specification allows a researcher to model the upper and lower tail dependence a distribution similar to the t-Copula. Unlike the t-Copula, though, the upper and lower-tail dependence are allowed to be asymmetric and differ in value. The third copula used this essay is a mixture of the regular Clayton\footnote{The Clayton copula is an Archimedean copula with generator function $\varphi\left(t\right) = (t^{-\theta} - 1) / \theta$} copula and its survival copula. These three copulas specifications---the t-Copula, the Symmetric Gumbel and the Symmetric Clayton---form the basis of our comparison. 

Table \ref{tbl:BIC_model_comparison} records the Bayesian Information Criterion (BIC) across the three different time-varying models of Section \ref{sec:time_varying_models} and the three different exchange rate pairs. The bolded values highlight the minimum BIC for each pair and model. The Sequential Break Point model is the only model of the three that produces the number of optimal regimes endogenously and its outcome provides a good cross-check to using the BIC for selecting the number of regimes to specify for the Markov Transition and Smooth Transition models. It is clear that the t-Copula is the preferred copula model, beating out the Symmetric Clayton and Symmetric Gumbel copulas across all three time varying models and all three exchange rate pairs under consideration. When it comes to the best time-varying model, the Markov Transition model outperforms the Smooth Transition and Sequential Breakpoint models for the Euro-Pound and Euro-Yen pairs while the Sequential Breakpoint model achieves the lowest BIC value for the Pound-Yen pair. Apart from dominance of the t-Copula, the Symmetric Gumbel copula fits the dependence structure of the exchange rate data better than the Symmetric Clayton copula. Interestingly, according to the BIC value, the two-state Markov Transition models perform better than the three-state and four-state options.

The dependency measures for the bolded models are contained in Tables \ref{tbl:dependence_measure_summary} while Tables \ref{tbl:regime_changes_euro_pound}, \ref{tbl:regime_changes_euro_yen}, and \ref{tbl:regime_changes_pound_yen} each record the dates of transition from one regime to another for each exchange rate pair. Figures \ref{fig:BPfit_dependence} - \ref{fig:STfit_dependence} record the evolution of the dependence measures for each of the three time-varying models under review. The Sequential Break Point model returns the day of each regime explicitly. For the Markov Transition model, the reported transition dates are the days the implied state probability of equation (\ref{eqn:Kim_smooth_probability}) crosses the 0.5 value in either direction. For the Smooth Transition model, the date implied by parameter $c$ from equation (\ref{eqn:smoothTrans}) is reported, but this date may be misleading since the pace of the transition, captured by $\gamma$ in equation (\ref{eqn:smoothTrans}), plays an important role as well. In certain situations, being able to estimate the pace of transition can be helpful. An overnight change in a relationship could indicate a response to a global economic shock while a slow but gradual change could be evidence of larger long-term economic forces such as the Law of One Price. These competing narratives can be seen playing out for the Pound-Yen pair in Figure \ref{fig:STfit_dependence}: a slow but gradual strengthening of the pair's dependence from 2000 to 2006, an abrupt reversal of that dependence in the early days of 2007\footnote{The extended dip into negative dependence from 2007 to 2009 is not a fact shared by the two competing dynamic models but it is representative of the data. Kendall's tau was calculated on a a rolling 60-day basis for the Pound/USD and Yen/USD exchange rate pair. A negative value of Kendall's tau was observed 927 times over the duration of the series with 421 of those values occurring in 2007, 2008, and 2009.}, and then a gradual return over the following ten years to a level of dependence similar to that observed in the beginning of 2000. The presence of gradual changes, though, can make it difficult to interpret the date of the regime change in a meaningful and chronological way. 

The unique properties of the Sequential Break Point model can be seen in its time-dependence structure. Unconditional copulas are estimated on the sub-samples demarcated by the the dates of each break. This explains the step-like nature of the dependence measures seen in Figure \ref{fig:BPfit_dependence}. All exchange rate pairs undergo three regime changes leading to four different periods, each defined by a different state of dependence. All three breaks occur before 2008 for each of the pairs. Overall dependence, as measured by Kendall's $\tau$, is increasing from 2000 to 2006 and then drops precipitously in 2007, where it remains unchanged until the end of the sample in 2018. By assessing the order of the break dates in Tables \ref{tbl:regime_changes_euro_pound}, \ref{tbl:regime_changes_euro_yen}, and \ref{tbl:regime_changes_pound_yen}, a general pattern emerges that a change in dependence with the Yen, compared to either European currency, is a leading indicator to a change in dependence between the Pound/USD and Euro/USD exchange rates.

When it comes to the Markov Transition models, the optimal number of regimes for each exchange rate pair is two. For pairs containing the Euro exchange rate there is one regime with high overall association---as judged by Kendall's $\tau$---with heavy tail dependence. The alternate regime has a reduced level of association and reduced tail dependence. One of the key features of the Markov Transition models used in this essay is their flexible nature. They can switch back-and-forth between their regimes on almost a daily basis, something that is not feasible with the Smooth Transition model due to its particular functional form. This characteristic of the model is easily seen in the markov transition columns of Tables \ref{tbl:regime_changes_euro_pound}, \ref{tbl:regime_changes_euro_yen}, and \ref{tbl:regime_changes_euro_pound}. The number of times the model iterates between the two regimes far exceeds the number of total regimes modeled by the other time-varying methods. Like the Sequential Break Point model, the Markov Transition model confirms the pattern of a change in dependence with the Yen/USD exchange rate precedes a change in dependence of the Euro/USD and Pound/USD exchange rates for dates up to the 2007 US mortgage backed crisis. That leading relationship breaks down, though, after 2008. The Euro-Pound pair enter a sustain period of reduced dependence from 2013 to 2017 while the dependence measures for pairs involving the Yen/USD exchange rate continue to oscillate between the two regimes.

\section{Conclusion} \label{sec:Conclusion}

This essay explored a set of tools that can be used study financial time series and model their bivariate distributions. These tools include the ARMA-GARCH framework for modeling the conditional mean and conditional volatility process of the exchange rates studied in this essay. The ARMA-GARCH models allow the researcher to control for asymmetry in response to positive versus negative returns and periods of volatility clustering. The use of Copulas allows for flexible distributional assumptions including excessive leptokurtosis, asymmetric tail dependencies, and departures from standard elliptical distributions. The use of the Sequential Break Point, Markov Transition, and Smooth Transition models provide different paths to model changes in the dependence structure over time. 

The utility of using these tools in conjunction with each other was demonstrated on three major global currencies and their exchange rate fluctuations with the US Dollar from 2000 to 2018.


\begin{landscape}

\begin{table}
	\caption{BIC Values for Estimated Copula Models}
	\fontsize{9pt}{9pt}\selectfont
	\centering
	\begin{tabular}[c]{l c r r r | r r r | r r r}
		\midrule
		            &        & \multicolumn{3}{c}{Markov Transition} & \multicolumn{3}{c}{Smooth Trans.} & \multicolumn{3}{c}{Seq. Break$^{1}$} \\
		\midrule
		Exchange Rate& Regime & tCop & S. Gumbel & S. Clayton & tCop & S. Gumbel & S. Clayton & tCop & S. Gumbel & S. Clayton \\
		\midrule
		Euro-Pound  & 1 &          -2634.1 & -2643.2 & -2556.0 &          -2634.1 & -2643.2 & -2556.0 &           -2634.1 & -2643.2 &  -2556.0  \\		
		            & 2 & \textbf{-2750.0} & -2727.7 & -2626.5 &          -2681.3 & -2672.5 & -2590.1 &                -- &      -- &  --       \\
					& 3 &          -2719.2 & -2676.0 & -2565.1 &          -2671.9 & -2654.3 & -2566.1 &                -- &      -- &  --       \\
					& 4 &          -2643.2 & -2593.0 & -2507.5 & \textbf{-2712.0} & -2699.8 & -2601.0 &  \textbf{-2748.1} & -2710.6 &  -2618.8  \\ \\
		Euro-Yen    & 1 &           -713.5 &  -682.8 &  -638.7 &           -713.5 &  -682.8 &  -638.7 &            -713.5 &  -682.8 &   -638.7  \\
		            & 2 &  \textbf{-939.8} &  -892.6 &  -868.5 &           -714.0 &  -686.3 &  -639.3 &                -- &      -- &  --       \\
				    & 3 &           -880.6 &  -825.3 &  -801.1 &           -772.5 &  -733.3 &  -701.4 &                -- &      -- &  --       \\
				    & 4 &           -805.5 &  -762.8 &  -743.2 &  \textbf{-853.8} &  -819.0 &  -781.9 &   \textbf{-840.6} &  -780.8 &   -746.4  \\ \\
		Pound-Yen   & 1 &           -365.1 &  -309.4 &  -273.5 &           -365.1 &  -309.4 &  -273.5 &            -365.1 &  -309.4 &   -273.4  \\
		            & 2 &  \textbf{-496.9} &  -424.1 &  -407.7 &           -416.0 &  -341.7 &  -312.8 &                -- &      -- &  --       \\
				    & 3 &           -470.2 &  -366.0 &  -365.6 &  \textbf{-481.5} &  -347.9 &  -325.5 &                -- &      -- &  --       \\
				    & 4 &           -406.8 &  -274.2 &  -298.5 &           -473.2 &  -397.1 &  -447.3 &   \textbf{-511.1} &  -418.0 &   -387.5  \\
		\hline
	\end{tabular}
	\begin{tablenotes}
		\item{This table records the Bayesian information criterion (BIC) for each model. The values in bold represent the minimum BIC value by exchange rate distribution and Markov Transition model.}
		\item{\footnotesize $^{1}$ The researcher does not need to provide the number of regimes to the Sequential Break Point model.}
	\end{tablenotes}
	\label{tbl:BIC_model_comparison}
\end{table}


\begin{table}
	\centering
	\caption{Estimated Dependence Measures from Fitted Models}
	\begin{tabular}[c]{l c r r r | r r r | r r r}
		\midrule
		         &        & \multicolumn{3}{c}{\underline{Markov Transition}} & \multicolumn{3}{c}{\underline{Smooth Transition}} & \multicolumn{3}{c}{\underline{Seq. Break Point}} \\
		Currency & Regime & $\tau$ & LTD & UTD & $\tau$ & LTD & UTD & $\tau$ & LTD & UTD \\
		\midrule
		Euro-Pound  & 1 &  0.558 & 0.255 & 0.255 &  0.155 & 0.004 & 0.004 &  0.381 & 0.111 & 0.111 \\
					& 2 &  0.368 & 0.059 & 0.059 &  0.972 & 0.972 & 0.972 &  0.527 & 0.226 & 0.226 \\
					& 3 &     -- &    -- &    -- &  0.529 & 0.154 & 0.154 &  0.621 & 0.524 & 0.524 \\
					& 4 &     -- &    -- &    -- &  0.385 & 0.072 & 0.072 &  0.412 & 0.099 & 0.099 \\ \\
		Euro-Yen    & 1 & -0.017 & 0.021 & 0.021 & -0.039 & 0.000 & 0.000 & -0.004 & 0.000 & 0.000 \\
					& 2 &  0.378 & 0.078 & 0.078 &  0.393 & 0.076 & 0.076 &  0.320 & 0.089 & 0.089 \\
					& 3 &     -- &    -- &    -- &  0.073 & 0.075 & 0.075 &  0.453 & 0.002 & 0.002 \\
					& 4 &     -- &    -- &    -- &  0.310 & 0.186 & 0.186 &  0.180 & 0.139 & 0.139 \\ \\
		Pound-Yen   & 1 &  0.030 & 0.041 & 0.041 &  0.102 & 0.023 & 0.023 &  0.020 & 0.004 & 0.004 \\
		            & 2 &  0.376 & 0.015 & 0.015 & -0.584 & 0.014 & 0.014 &  0.258 & 0.069 & 0.069 \\
		            & 3 &     -- &    -- &    -- &  0.183 & 0.084 & 0.084 &  0.439 & 0.002 & 0.002 \\
		            & 4 &     -- &    -- &    -- &     -- &    -- &    -- &  0.073 & 0.071 & 0.071 \\
		\hline
	\end{tabular}
	\begin{tablenotes}
		\item{\footnotesize This table summarizes the changes of tail dependency and Kendall's $\tau$ as estimated from the Markov Transition, Smooth Transition, and Sequential Breakpoint models. The values of Kendall's $\tau$ and tail dependence come from the models that minimize the BIC value. This is recorded in Table 4.}
	\end{tablenotes}
	\label{tbl:dependence_measure_summary}
\end{table}

\end{landscape}

\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{BPfit_dependence.png}
    \caption{This figure captures the time-varying dependence from the Sequential Break Point model. The top figure shows the change in Kendall's $\tau$ while the bottom figure shows the change in tail dependence of the estimated t-Copula. The unique properties of the model can be seen in the time-dependence structure. Unconditional copulas are estimated on the sub-samples demarcated by the the dates of each break. This explains the step-like nature of the evolution of the dependence measures. All three exchange rate pairs undergo three breakpoints creating four different dependence regimes with all three breaks occurring before 2008. Overall dependence, as measured by Kendall's $\tau$, is increasing from 2000 to 2006 and then drops precipitous in 2007, where it remains unchanged until the end of the sample in 2018.}
    \label{fig:BPfit_dependence}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{MSfit_dependence.png}
    \caption{This figure captures the time-varying dependence from the Markov Transition model. The top figure shows the change in Kendall's $\tau$ while the bottom figure shows the change in tail dependence of the estimated t-Copula. The unique markov property of the model can be seen in the time-dependence structure. Instead of three or four different regimes recorded by the Sequential Break Point and Smooth Transition models, the Markov Transition model selects two separate regimes and iterates back and forth between the two over the duration of the timeframe.}
    \label{fig:MSfit_dependence}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{STfit_dependence.png}
    \caption{This figure captures the time-varying dependence from the Smooth Transition model. The top figure shows the change in Kendall's $\tau$ while the bottom figure shows the change in tail dependence of the estimated t-Copula. The unique properties of the model can be seen in the time-dependence structure. Unlike the Markov Transition model, whose day-to-day dependence measures can vary wildly between the different regimes, the dependence measure of the Smooth Transition models have a more fluid evolution.}
    \label{fig:STfit_dependence}
\end{figure}

\begin{table}
	\centering
	\caption{Dates of Regime Change: Euro-Pound}
	\begin{tabular}[c]{r r | r r | r r}
		\midrule
		\multicolumn{2}{c}{Markov Transition} & \multicolumn{2}{c}{Smooth Trans.} & \multicolumn{2}{c}{Seq. Break} \\
		\midrule
		Date & Regime & Date & Regime & Date & Regime \\
		\midrule
		2000-01-04 &  1 & 2000-01-04 &  1 & 2000-01-04 &  1 \\
		2000-02-03 &  2 &         -- &  1 &         -- &  1 \\
		        -- &  2 & 2000-12-06 &  2 &         -- &  1 \\
		2001-08-16 &  1 &         -- &  2 &         -- &  1 \\
		        -- &  1 &         -- &  2 & 2001-10-11 &  2 \\
		2003-01-30 &  2 &         -- &  2 &         -- &  2 \\
		2003-10-17 &  1 &         -- &  2 &         -- &  2 \\
		        -- &  1 &         -- &  2 & 2004-12-29 &  3 \\
				-- &  1 & 2007-04-17 &  3 &         -- &  3 \\
		        -- &  1 &         -- &  3 & 2007-07-13 &  4 \\				
		2007-09-11 &  2 &         -- &  3 &         -- &  4 \\
		2008-06-12 &  1 &         -- &  3 &         -- &  4 \\
		2008-11-11 &  2 &         -- &  3 &         -- &  4 \\
		2009-03-25 &  1 &         -- &  3 &         -- &  4 \\
		2009-08-26 &  2 &         -- &  3 &         -- &  4 \\
		2011-06-09 &  1 &         -- &  3 &         -- &  4 \\
		2012-12-31 &  2 &         -- &  3 &         -- &  4 \\
		        -- &  2 & 2013-01-08 &  4 &         -- &  4 \\
		2017-12-13 &  1 &         -- &  4 &         -- &  4 \\
		\hline
	\end{tabular}
	\begin{tablenotes}
		\item{\footnotesize This table records the dates of regime switching as indicated by each model. The Sequential Break Point model returns the day of each regime explicitly. For the Smooth Transition model the date implied by parameter $c$ from equation (\ref{eqn:smoothTrans}) is reported. For the Markov Transition model, the reported regime switching dates are the days the implied state probability of equation (\ref{eqn:Kim_smooth_probability}) crosses the 0.5 value in either direction.}
	\end{tablenotes}
	\label{tbl:regime_changes_euro_pound}
\end{table}


\begin{table}
	\centering
	\caption{Dates of Regime Change: Euro-Yen}
	\begin{tabular}[c]{r r | r r | r r}
		\midrule
		\multicolumn{2}{c}{Markov Transition} & \multicolumn{2}{c}{Smooth Trans.} & \multicolumn{2}{c}{Seq. Break} \\
		\midrule
		Date & Regime & Date & Regime & Date & Regime \\
		\midrule
		2000-01-04 &  1 & 2000-01-04 &  1 & 2000-01-04 &  1 \\
		        -- &  1 &         -- &  1 & 2001-03-02 &  2 \\
		2001-06-11 &  2 &         -- &  1 &         -- &  2 \\
		        -- &  2 & 2001-07-03 &  2 &         -- &  2 \\
		2001-12-12 &  1 &         -- &  2 &         -- &  2 \\
		2002-02-20 &  2 &         -- &  2 &         -- &  2 \\
		2003-08-01 &  1 &         -- &  2 &         -- &  2 \\
		2003-09-19 &  2 &         -- &  2 &         -- &  2 \\
		        -- &  2 &         -- &  2 & 2004-07-02 &  3 \\
				-- &  2 &         -- &  2 & 2007-01-03 &  4 \\
				-- &  2 & 2007-02-23 &  3 &         -- &  4 \\
		2007-05-29 &  1 &         -- &  3 &         -- &  4 \\
		2008-02-13 &  2 &         -- &  3 &         -- &  4 \\
		2008-08-13 &  1 &         -- &  3 &         -- &  4 \\
		2009-06-04 &  2 &         -- &  3 &         -- &  4 \\
		2009-06-10 &  1 &         -- &  3 &         -- &  4 \\
		2009-09-08 &  2 &         -- &  3 &         -- &  4 \\
		2009-10-12 &  1 &         -- &  3 &         -- &  4 \\
		2009-12-10 &  2 &         -- &  3 &         -- &  4 \\
		2010-01-15 &  1 &         -- &  3 &         -- &  4 \\
		2010-07-19 &  2 &         -- &  3 &         -- &  4 \\
		2010-08-27 &  1 &         -- &  3 &         -- &  4 \\
		2010-09-07 &  2 &         -- &  3 &         -- &  4 \\
		2011-03-09 &  1 &         -- &  3 &         -- &  4 \\
		2011-10-20 &  2 &         -- &  3 &         -- &  4 \\
		2012-01-27 &  1 &         -- &  3 &         -- &  4 \\
		        -- &  1 & 2013-04-26 &  4 &         -- &  4 \\
		2013-04-30 &  2 &         -- &  4 &         -- &  4 \\
		2013-11-08 &  1 &         -- &  4 &         -- &  4 \\
		2014-03-18 &  2 &         -- &  4 &         -- &  4 \\
		2016-06-08 &  1 &         -- &  4 &         -- &  4 \\
		2016-07-18 &  2 &         -- &  4 &         -- &  4 \\
		2017-04-18 &  1 &         -- &  4 &         -- &  4 \\
		2017-05-05 &  2 &         -- &  4 &         -- &  4 \\
		2018-05-16 &  1 &         -- &  4 &         -- &  4 \\
		\hline
	\end{tabular}
	\begin{tablenotes}
		\item{\footnotesize This table records the dates of regime switching as indicated by each model. The Sequential Break Point model returns the day of each regime explicitly. For the Smooth Transition model the date implied by parameter $c$ from equation (\ref{eqn:smoothTrans}) is reported. For the Markov Transition model, the reported regime switching dates are the days the implied state probability of equation (\ref{eqn:Kim_smooth_probability}) crosses the 0.5 value in either direction.}
	\end{tablenotes}
	\label{tbl:regime_changes_euro_yen}
\end{table}


\begin{table}
	\centering
	\caption{Dates of Regime Change: Pound-Yen}
	\begin{tabular}[c]{r r | r r | r r}
		\midrule
		\multicolumn{2}{c}{Markov Transition} & \multicolumn{2}{c}{Smooth Trans.} & \multicolumn{2}{c}{Seq. Break} \\
		\midrule
		Date & Regime & Date & Regime & Date & Regime \\
		\midrule
		2000-01-04 &  1 & 2000-01-04 &  1 & 2000-01-04 &  1 \\
		        -- &  1 &         -- &  1 & 2001-06-21 &  2 \\
		2001-10-11 &  2 &         -- &  1 &         -- &  2 \\
		2001-12-04 &  1 &         -- &  1 &         -- &  2 \\
		2002-04-09 &  2 &         -- &  1 &         -- &  2 \\
		2003-02-12 &  1 &         -- &  1 &         -- &  2 \\
		2003-03-24 &  2 &         -- &  1 &         -- &  2 \\
		2003-04-25 &  1 &         -- &  1 &         -- &  2 \\
		2004-02-12 &  2 &         -- &  1 &         -- &  2 \\
		        -- &  2 &         -- &  1 & 2004-04-06 &  3 \\
		2006-12-22 &  1 &         -- &  1 &         -- &  3 \\
		        -- &  1 &         -- &  1 & 2007-01-03 &  4 \\
		        -- &  1 & 2007-01-08 &  2 &         -- &  4 \\
				-- &  1 & 2007-01-09 &  3 &         -- &  4 \\
		2007-04-30 &  2 &         -- &  3 &         -- &  4 \\
		2007-05-23 &  1 &         -- &  3 &         -- &  4 \\
		2008-06-10 &  2 &         -- &  3 &         -- &  4 \\
		2008-07-23 &  1 &         -- &  3 &         -- &  4 \\
		2010-10-04 &  2 &         -- &  3 &         -- &  4 \\
		2011-01-20 &  1 &         -- &  3 &         -- &  4 \\
		2011-11-22 &  2 &         -- &  3 &         -- &  4 \\
		2012-01-13 &  1 &         -- &  3 &         -- &  4 \\
		2013-04-29 &  2 &         -- &  3 &         -- &  4 \\
		2013-09-02 &  1 &         -- &  3 &         -- &  4 \\
		2013-09-04 &  2 &         -- &  3 &         -- &  4 \\
		2013-11-08 &  1 &         -- &  3 &         -- &  4 \\
		2014-10-27 &  2 &         -- &  3 &         -- &  4 \\
		2015-01-19 &  1 &         -- &  3 &         -- &  4 \\
		2015-02-09 &  2 &         -- &  3 &         -- &  4 \\
		2015-06-30 &  1 &         -- &  3 &         -- &  4 \\
		2015-09-24 &  2 &         -- &  3 &         -- &  4 \\
		2015-12-14 &  1 &         -- &  3 &         -- &  4 \\
		2016-12-23 &  2 &         -- &  3 &         -- &  4 \\
		2017-03-30 &  1 &         -- &  3 &         -- &  4 \\
		\hline
	\end{tabular}
	\begin{tablenotes}
		\item{\footnotesize This table records the dates of regime switching as indicated by each model. The Sequential Break Point model returns the day of each regime explicitly. For the Smooth Transition model the date implied by parameter $c$ from equation (\ref{eqn:smoothTrans}) is reported. For the Markov Transition model, the reported regime switching dates are the days the implied state probability of equation (\ref{eqn:Kim_smooth_probability}) crosses the 0.5 value in either direction.}
	\end{tablenotes}
	\label{tbl:regime_changes_pound_yen}
\end{table}

\clearpage

\printbibliography

\end{document}